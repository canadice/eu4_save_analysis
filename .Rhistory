var <- orig  <- 100
var <- orig  <- 100
for(i in 1:100){
var <- var ^ 1.0002
}
var/orig
(var-orig)/orig
var <- orig  <- 2
for(i in 1:100){
var <- var ^ 1.0002
}
(var-orig)/orig
var <- orig  <- 2
for(i in 1:100){
var <- var * 1.0002
}
(var-orig)/orig
var <- orig  <- 100
for(i in 1:100){
var <- var * 1.0002
}
(var-orig)/orig
var <- orig  <- 200
for(i in 1:100){
var <- var * 1.0002
}
(var-orig)/orig
shiny::runApp('F:/GitHubs/DnD_Hex_Map_Creator')
require(shiny)
require(shinycssloaders, quietly = TRUE)
require(DT, quietly = TRUE)
require(shiny, quietly = TRUE)
require(shinythemes, quietly = TRUE)
require(stringr)
require(dichromat)
install.packages("dichromat")
runApp('F:/GitHubs/DnD_Hex_Map_Creator')
runApp('F:/GitHubs/DnD_Hex_Map_Creator')
runApp('F:/GitHubs/DnD_Hex_Map_Creator')
runApp('F:/GitHubs/DnD_Hex_Map_Creator')
runApp('F:/GitHubs/DnD_Hex_Map_Creator')
runApp('F:/GitHubs/DnD_Hex_Map_Creator')
runApp('F:/GitHubs/DnD_Hex_Map_Creator')
runApp('F:/GitHubs/DnD_Hex_Map_Creator')
runApp('F:/GitHubs/DnD_Hex_Map_Creator')
runApp('F:/GitHubs/DnD_Hex_Map_Creator')
runApp('F:/GitHubs/DnD_Hex_Map_Creator')
runApp('F:/GitHubs/DnD_Hex_Map_Creator')
runApp('F:/GitHubs/DnD_Hex_Map_Creator')
runApp('F:/GitHubs/DnD_Hex_Map_Creator')
runApp('F:/GitHubs/DnD_Hex_Map_Creator')
options(shiny.maxRequestSize = 60 * 1024 ^ 2)
setwd("F:/GitHubs/eu4_save_analysis")
source("save_scraper.R")
source("data_compiler.R")
require(shiny, quietly = TRUE)
require(shinythemes, quietly = TRUE)
require(stringr, quietly = TRUE)
require(ggplot2, quietly = TRUE)
require(dplyr, quietly = TRUE)
require(tidyr, quietly = TRUE)
require(stringr, quietly = TRUE)
require(parallel, quietly = TRUE)
require(shinycssloaders, quietly = TRUE)
require(DT, quietly = TRUE)
ui <-
fluidPage(
theme = shinytheme("united"),
title = "Save analysis for Europa Universalis 4",
## Adds a title to the page containing the meta-information about the currently loaded save
uiOutput(outputId = "header"),
## The panel of options present in the app
navbarPage(
"Functions",
#### Adds a tab containing Import options for new saves ####
tabPanel("Import New Save",
fluidRow(column(
width = 2,
wellPanel(
actionButton(inputId = "data_choice",
label = "Upload uncompressed .eu4"),
br(),
br(),
withSpinner(uiOutput("upload"))
)
))),
#### Adds a tab containing pie-chart comparisons of two groups of nations ####
tabPanel("Group Comparisons",
fluidRow(
column(
width = 2,
wellPanel(
## Choice of names and nations for grouping
withSpinner(uiOutput("own_team_name1")),
withSpinner(uiOutput("attacking")),
withSpinner(uiOutput("own_team_name2")),
withSpinner(uiOutput("defending"))
)
),
column(width = 10,
withSpinner(
## Visualization
plotOutput("pie_chart_facet_comparison")
))
)),
#### Adds a tab giving table visualizations of province data ####
tabPanel(
"Province Information",
column(width = 2,
wellPanel(uiOutput(
"data_province_vars"
))),
column(width = 10,
fluidRow(column(
width = 12,
withSpinner(dataTableOutput("province_data"))
)))
),
#### Adds a tab giving table visualizations of nation data ####
tabPanel("Nation Information",
fluidRow(
column(width = 2,
wellPanel(
## Allows for selection of specific nations
withSpinner(uiOutput("nations")),
uiOutput("data_nation_vars")
)),
column(width = 10,
fluidRow(column(
width = 12,
withSpinner(dataTableOutput("country_data"))
)))
))
#### End of Panels ####
)
)
save <- readLines(con = "C:/Users/Canadice/Documents/Paradox Interactive/Europa Universalis IV/save games/Thunderdome Session 6.eu4", encoding = "ANSI", warn = FALSE)
tags <- read.csv2("tags.csv")
tags
save
end <- which(str_detect(save, pattern = "^trade"))[1]
end
meta <- save[1:(end - 1)]
meta
start_country <- which(str_detect(save, pattern = "^countries=\\{"))
start_country
country_data <- save[start_country:(which(str_detect(save, pattern = "active_advisors=\\{"))-1)]
country_data
end <- which(str_detect(save, pattern = "^map_area_data"))[1]
meta <- save[1:(end - 1)]
meta
area_end <- which(str_detect(save, pattern = "^total_military_power"))[1]
area_data <- save[meta_end:(area_end-1)]
meta_end <- which(str_detect(save, pattern = "^map_area_data"))[1]
meta <- save[1:(meta_end - 1)]
area_end <- which(str_detect(save, pattern = "^total_military_power"))[1]
area_data <- save[meta_end:(area_end-1)]
start_country <- which(str_detect(save, pattern = "^countries=\\{"))
start_country
starts_provinces <- which(str_detect(save, pattern = "^-[0-9]+=\\{"))
starts_provinces
ends_provinces <- c((starts_provinces-1)[-1], start_country-1)
indices <- do.call(list, mapply(seq, starts_provinces, ends_provinces))
indices
province_data_split <- lapply(indices, FUN = function(x){save[x]})
province_data_split
start_country <- which(str_detect(save, pattern = "^countries=\\{"))
country_data <- save[start_country:(which(str_detect(save, pattern = "active_advisors=\\{"))-1)]
country_data
starts <- which(str_detect(country_data, pattern = "^\t[A-Z]{3}=\\{"))
ends <- c((starts-1)[-1], length(country_data))
indices <- do.call(list, mapply(seq, starts, ends))
country_data_split <- lapply(indices, FUN = function(x){country_data[x]})
rm(list = c("ends", "end", "starts", "starts_provinces", "ends_provinces", "start_country", "indices", "save"))
rm(list = c("meta_end", "area_end", "ends", "end", "starts", "starts_provinces", "ends_provinces", "start_country", "indices", "save"))
meta_data <- meta_information_scraper(vector = meta)
meta_data
cl <- makeCluster(getOption("cl.cores", 4))
clusterExport(cl, varlist = c("information_finder", "province_information_scraper"))
data <- parLapply(cl = cl, X = province_data_split, fun = province_information_compiler)
stopCluster(cl)
province_data <- data %>%
Reduce(function(dtf1,dtf2) suppressWarnings(bind_rows(dtf1,dtf2)), .)
province_data$development <- sum(province_data$base_tax, province_data$base_manpower, province_data$base_production)
buildings <- which(colnames(province_data) %in% c("marketplace", "workshop", "temple", "barracks", "shipyard", "fort_15th",
"courthouse", "dock", "regimental_camp", "fort_16th",
"cathedral", "university", "trade_depot", "grand_shipyard", "training_fields", "fort_17th",
"stock_exchange", "counting_house", "town_hall", "drydock", "conscription_center", "fort_18th",
"wharf", "weapons", "textile", "plantations", "tradecompany")
)
fort <- which(str_detect(colnames(x = province_data), pattern = "^fort_inf"))
cores <- which(str_detect(colnames(x = province_data), pattern = "^core"))
claims <- which(str_detect(colnames(x = province_data), pattern = "^claim"))
base <- which(str_detect(colnames(x = province_data), pattern = "^base") | str_detect(colnames(x = province_data), pattern = "development"))
PID <- which(str_detect(colnames(x = province_data), pattern = "^PID"))
originals <- which(str_detect(colnames(x = province_data), pattern = "^original"))
info <- which(colnames(province_data) %in% c("name", "culture", "religion", "capital", "trade_goods", "trade_power",
"trade", "local_autonomy", "hre", "owner")
)
ordering_index <- c(PID, info, base, buildings, fort, cores, claims, originals)
province_data <- province_data[,c(ordering_index, (1:ncol(province_data))[-ordering_index])]
View(province_data)
cl <- makeCluster(getOption("cl.cores", 4))
clusterExport(cl, varlist = c("information_finder"))
data <- parLapply(cl = cl, X = country_data_split, fun = country_information_compiler)
stopCluster(cl)
# Takes all information in the list and concatenate into a data frame
country_data <- data %>%
Reduce(function(dtf1,dtf2) suppressWarnings(bind_rows(dtf1,dtf2)), .)
View(country_data)
country_data <- country_data %>% left_join(province_data[, c("PID", "hre")], by = c("capital" = "PID"))
country_data <- country_data %>% inner_join(tags, by = c("tag" = "Tag"))
ind <- c("tag", "Name")
country_data <- country_data[, c(ind, colnames(country_data)[!colnames(country_data) %in% ind])]
country_data <- country_data[which(country_data$capped_development > 0),]
resulting_data <- list(meta = meta_data, province = province_data, country = country_data)
game_data <- save_processing(save)
save <- readLines(con = "C:/Users/Canadice/Documents/Paradox Interactive/Europa Universalis IV/save games/Thunderdome Session 6.eu4", encoding = "ANSI", warn = FALSE)
game_data <- save_processing(save)
save <- readLines(con = "C:/Users/Canadice/Documents/Paradox Interactive/Europa Universalis IV/save games/Thunderdome Session 6.eu4", encoding = "ANSI", warn = FALSE)
############################################################
### Basic options
############################################################
## Increasing the possible allowed size for uploaded save-files.
options(shiny.maxRequestSize = 60 * 1024 ^ 2)
## Setting the WD to the local directory of the Git.
setwd("F:/GitHubs/eu4_save_analysis")
# Sourcing the scraper and compiler scripts
source("save_scraper.R")
source("data_compiler.R")
## Loading the required packages
require(shiny, quietly = TRUE)
require(shinythemes, quietly = TRUE)
require(stringr, quietly = TRUE)
require(ggplot2, quietly = TRUE)
require(dplyr, quietly = TRUE)
require(tidyr, quietly = TRUE)
require(stringr, quietly = TRUE)
require(parallel, quietly = TRUE)
require(shinycssloaders, quietly = TRUE)
require(DT, quietly = TRUE)
# Sources the different functions needed
source("data_compiler.R")
# Imports a list of tags and responding names of nation
tags <- read.csv2("tags.csv")
##########################################################################
### Subsetting
##########################################################################
############################
### META
############################
### Starts the splitting for meta-data
# The first part of data existing in the save is area data, meta-data is located above
meta_end <- which(str_detect(save, pattern = "^map_area_data"))[1]
meta <- save[1:(meta_end - 1)]
############################
### Area data
############################
### Starts the splitting for area data
area_end <- which(str_detect(save, pattern = "^total_military_power"))[1]
area_data <- save[meta_end:(area_end-1)]
############################
### Country
############################
### Splits the save file into the nations parts, starting at the first nation
### Last list object contains the last nation information + all the rest of the save...
# Detects where to start looking for country data
start_country <- which(str_detect(save, pattern = "^countries=\\{"))
# Detects position of } as it defines end of information block.
# As the game progresses, } without any tabs are inserted resulting in the following
# solution to make the last list-object takes the whole rest of the save to the end.
# Subsets just the country data, might be unecessary.
# 1.23 added active advisors to the save which screws with finding positions of nations' tags
country_data <- save[start_country:(which(str_detect(save, pattern = "active_advisors=\\{"))-1)]
starts <- which(str_detect(country_data, pattern = "^\t[A-Z]{3}=\\{"))
ends <- c((starts-1)[-1], length(country_data))
indices <- do.call(list, mapply(seq, starts, ends))
country_data_split <- lapply(indices, FUN = function(x){country_data[x]})
############################
### Province
############################
### Splits the save file into the province parts, starting at the first province
### Last list object contains the last province information + all the rest of the save...
# Detects where to start the individual province data
starts_provinces <- which(str_detect(save, pattern = "^-[0-9]+=\\{"))
# As starts portion off the different provinces, ends are the one line before
# The list of provinces end where countries start so the last province will end on the line before
ends_provinces <- c((starts_provinces-1)[-1], start_country-1)
# Splits the data into a list of their individual province information
indices <- do.call(list, mapply(seq, starts_provinces, ends_provinces))
province_data_split <- lapply(indices, FUN = function(x){save[x]})
############################
### Cleaning
############################
# Removes original save and indices to save at least some working space
rm(list = c("meta_end", "area_end", "ends", "end", "starts", "starts_provinces", "ends_provinces", "start_country", "indices", "save"))
##########################################################################
### Structuring & scraping
##########################################################################
############################
### Meta data
############################
meta_data <- meta_information_scraper(vector = meta)
############################
### Provinces data
############################
# Compiles the province data to a list, parallel processing to speed the list up
cl <- makeCluster(getOption("cl.cores", 4))
clusterExport(cl, varlist = c("information_finder", "province_information_scraper"))
data <- parLapply(cl = cl, X = province_data_split, fun = province_information_compiler)
stopCluster(cl)
# Takes all information in the list and concatenate into a data frame
province_data <- data %>%
Reduce(function(dtf1,dtf2) suppressWarnings(bind_rows(dtf1,dtf2)), .)
###############
### Development
###############
province_data$development <- sum(province_data$base_tax, province_data$base_manpower, province_data$base_production)
###############
### Structure
###############
# Finds the column index for sorting
buildings <- which(colnames(province_data) %in% c("marketplace", "workshop", "temple", "barracks", "shipyard", "fort_15th",
"courthouse", "dock", "regimental_camp", "fort_16th",
"cathedral", "university", "trade_depot", "grand_shipyard", "training_fields", "fort_17th",
"stock_exchange", "counting_house", "town_hall", "drydock", "conscription_center", "fort_18th",
"wharf", "weapons", "textile", "plantations", "tradecompany")
)
fort <- which(str_detect(colnames(x = province_data), pattern = "^fort_inf"))
cores <- which(str_detect(colnames(x = province_data), pattern = "^core"))
claims <- which(str_detect(colnames(x = province_data), pattern = "^claim"))
base <- which(str_detect(colnames(x = province_data), pattern = "^base") | str_detect(colnames(x = province_data), pattern = "development"))
PID <- which(str_detect(colnames(x = province_data), pattern = "^PID"))
originals <- which(str_detect(colnames(x = province_data), pattern = "^original"))
info <- which(colnames(province_data) %in% c("name", "culture", "religion", "capital", "trade_goods", "trade_power",
"trade", "local_autonomy", "hre", "owner")
)
ordering_index <- c(PID, info, base, buildings, fort, cores, claims, originals)
province_data <- province_data[,c(ordering_index, (1:ncol(province_data))[-ordering_index])]
############################
### Country data
############################
# Compiles the country data to a list
cl <- makeCluster(getOption("cl.cores", 4))
clusterExport(cl, varlist = c("information_finder"))
data <- parLapply(cl = cl, X = country_data_split, fun = country_information_compiler)
stopCluster(cl)
# Takes all information in the list and concatenate into a data frame
country_data <- data %>%
Reduce(function(dtf1,dtf2) suppressWarnings(bind_rows(dtf1,dtf2)), .)
# Merges with the province data for each of the countries' capitals
country_data <- country_data %>% left_join(province_data[, c("PID", "hre")], by = c("capital" = "PID"))
# Merges with the country name data from tags
country_data <- country_data %>% inner_join(tags, by = c("tag" = "Tag"))
ind <- c("tag", "Name")
country_data <- country_data[, c(ind, colnames(country_data)[!colnames(country_data) %in% ind])]
# Subsets nations that exist at the current save
country_data <- country_data[which(country_data$capped_development > 0),]
resulting_data <- list(meta = meta_data, province = province_data, country = country_data)
############################################################
### Basic options
############################################################
## Increasing the possible allowed size for uploaded save-files.
options(shiny.maxRequestSize = 60 * 1024 ^ 2)
## Setting the WD to the local directory of the Git.
setwd("F:/GitHubs/eu4_save_analysis")
# Sourcing the scraper and compiler scripts
source("save_scraper.R")
source("data_compiler.R")
## Loading the required packages
require(shiny, quietly = TRUE)
require(shinythemes, quietly = TRUE)
require(stringr, quietly = TRUE)
require(ggplot2, quietly = TRUE)
require(dplyr, quietly = TRUE)
require(tidyr, quietly = TRUE)
require(stringr, quietly = TRUE)
require(parallel, quietly = TRUE)
require(shinycssloaders, quietly = TRUE)
require(DT, quietly = TRUE)
############################################################
## Increasing the possible allowed size for uploaded save-files.
options(shiny.maxRequestSize = 60 * 1024 ^ 2)
## Setting the WD to the local directory of the Git.
setwd("F:/GitHubs/eu4_save_analysis")
# Sourcing the scraper and compiler scripts
source("save_scraper.R")
source("data_compiler.R")
## Loading the required packages
require(shiny, quietly = TRUE)
require(shinythemes, quietly = TRUE)
require(stringr, quietly = TRUE)
require(ggplot2, quietly = TRUE)
require(dplyr, quietly = TRUE)
require(tidyr, quietly = TRUE)
require(stringr, quietly = TRUE)
require(parallel, quietly = TRUE)
require(shinycssloaders, quietly = TRUE)
require(DT, quietly = TRUE)
##
save <- readLines(con = "C:/Users/Canadice/Documents/Paradox Interactive/Europa Universalis IV/save games/Thunderdome Session 6.eu4", encoding = "ANSI", warn = FALSE)
game_data <- save_processing(save)
save(game_data, file = "latest.RData")
runApp()
# Sources the different functions needed
source("data_compiler.R")
# Imports a list of tags and responding names of nation
tags <- read.csv2("tags.csv")
##########################################################################
### Subsetting
##########################################################################
############################
### META
############################
### Starts the splitting for meta-data
# The first part of data existing in the save is area data, meta-data is located above
meta_end <- which(str_detect(save, pattern = "^map_area_data"))[1]
meta <- save[1:(meta_end - 1)]
############################
### Area data
############################
### Starts the splitting for area data
area_end <- which(str_detect(save, pattern = "^total_military_power"))[1]
area_data <- save[meta_end:(area_end-1)]
############################
### Country
############################
### Splits the save file into the nations parts, starting at the first nation
### Last list object contains the last nation information + all the rest of the save...
# Detects where to start looking for country data
start_country <- which(str_detect(save, pattern = "^countries=\\{"))
# Detects position of } as it defines end of information block.
# As the game progresses, } without any tabs are inserted resulting in the following
# solution to make the last list-object takes the whole rest of the save to the end.
# Subsets just the country data, might be unecessary.
# 1.23 added active advisors to the save which screws with finding positions of nations' tags
country_data <- save[start_country:(which(str_detect(save, pattern = "active_advisors=\\{"))-1)]
starts <- which(str_detect(country_data, pattern = "^\t[A-Z]{3}=\\{"))
ends <- c((starts-1)[-1], length(country_data))
indices <- do.call(list, mapply(seq, starts, ends))
country_data_split <- lapply(indices, FUN = function(x){country_data[x]})
############################
### Province
############################
### Splits the save file into the province parts, starting at the first province
### Last list object contains the last province information + all the rest of the save...
# Detects where to start the individual province data
starts_provinces <- which(str_detect(save, pattern = "^-[0-9]+=\\{"))
# As starts portion off the different provinces, ends are the one line before
# The list of provinces end where countries start so the last province will end on the line before
ends_provinces <- c((starts_provinces-1)[-1], start_country-1)
# Splits the data into a list of their individual province information
indices <- do.call(list, mapply(seq, starts_provinces, ends_provinces))
province_data_split <- lapply(indices, FUN = function(x){save[x]})
############################
### Cleaning
############################
# Removes original save and indices to save at least some working space
rm(list = c("meta_end", "area_end", "ends", "starts", "starts_provinces", "ends_provinces", "start_country", "indices", "save"))
##########################################################################
### Structuring & scraping
##########################################################################
############################
### Meta data
############################
meta_data <- meta_information_scraper(vector = meta)
############################
### Provinces data
############################
# Compiles the province data to a list, parallel processing to speed the list up
cl <- makeCluster(getOption("cl.cores", 4))
clusterExport(cl, varlist = c("information_finder", "province_information_scraper"))
data <- parLapply(cl = cl, X = province_data_split, fun = province_information_compiler)
stopCluster(cl)
# Takes all information in the list and concatenate into a data frame
province_data <- data %>%
Reduce(function(dtf1,dtf2) suppressWarnings(bind_rows(dtf1,dtf2)), .)
###############
province_data$development <- sum(province_data$base_tax, province_data$base_manpower, province_data$base_production)
province_data$base_tax
province_data$base_manpower
sum(province_data$base_tax, province_data$base_manpower, province_data$base_production)
rowsum(province_data$base_tax, province_data$base_manpower, province_data$base_production)
province_data$base_production
province_data$development <- province_data$base_tax + province_data$base_manpower + province_data$base_production
############################################################
### Basic options
############################################################
## Increasing the possible allowed size for uploaded save-files.
options(shiny.maxRequestSize = 60 * 1024 ^ 2)
## Setting the WD to the local directory of the Git.
setwd("F:/GitHubs/eu4_save_analysis")
# Sourcing the scraper and compiler scripts
source("save_scraper.R")
source("data_compiler.R")
## Loading the required packages
require(shiny, quietly = TRUE)
require(shinythemes, quietly = TRUE)
require(stringr, quietly = TRUE)
require(ggplot2, quietly = TRUE)
require(dplyr, quietly = TRUE)
require(tidyr, quietly = TRUE)
require(stringr, quietly = TRUE)
require(parallel, quietly = TRUE)
require(shinycssloaders, quietly = TRUE)
require(DT, quietly = TRUE)
##
save <- readLines(con = "C:/Users/Canadice/Documents/Paradox Interactive/Europa Universalis IV/save games/Thunderdome Session 6.eu4", encoding = "ANSI", warn = FALSE)
game_data <- save_processing(save)
save(game_data, file = "latest.RData")
runApp()
runApp()
